
-----------START-----------

label_paths : [['./data/train/LGG/Brats18_2013_0_1/Brats18_2013_0_1_seg.nii.gz']]
making train tensor...
.
BraTS2018.py:69: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).

* deprecated from version: 3.0
* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0
  np_img_data = np.array(img_data.get_data())
BraTS2018.py:81: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).

* deprecated from version: 3.0
* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0
  np_img_data = np.array(img_data.get_data())
torch.Size([4, 240, 240, 128])
input shape : torch.Size([1, 4, 240, 240, 128])

conv1_1 shape : torch.Size([1, 8, 240, 240, 128])
conv1_2 shape : torch.Size([1, 8, 240, 240, 128])
pool1 shape : torch.Size([1, 8, 120, 120, 64])

conv2_1 shape : torch.Size([1, 16, 120, 120, 64])
conv2_2 shape : torch.Size([1, 16, 120, 120, 64])
pool2 shape : torch.Size([1, 16, 60, 60, 32])

conv5_1 shape : torch.Size([1, 128, 15, 15, 8])
conv5_2 shape : torch.Size([1, 128, 15, 15, 8])

deconv6 shape : torch.Size([1, 64, 30, 30, 16])
concat6 shape : torch.Size([1, 128, 30, 30, 16])
conv6_1 shape : torch.Size([1, 64, 30, 30, 16])
conv6_2 shape : torch.Size([1, 64, 30, 30, 16])

conv9_2 shape : torch.Size([1, 8, 240, 240, 128])

output shape : torch.Size([1, 1, 240, 240, 128])
tensor([[[[[0.0085, 0.0073, 0.0083,  ..., 0.0068, 0.0074, 0.0075],
           [0.0082, 0.0072, 0.0083,  ..., 0.0072, 0.0081, 0.0074],
           [0.0082, 0.0072, 0.0085,  ..., 0.0076, 0.0086, 0.0066],
           ...,
           [0.0080, 0.0069, 0.0089,  ..., 0.0069, 0.0089, 0.0069],
           [0.0079, 0.0073, 0.0081,  ..., 0.0071, 0.0089, 0.0064],
           [0.0085, 0.0073, 0.0089,  ..., 0.0072, 0.0088, 0.0067]],

          [[0.0078, 0.0078, 0.0069,  ..., 0.0088, 0.0067, 0.0072],
           [0.0071, 0.0077, 0.0078,  ..., 0.0087, 0.0075, 0.0079],
           [0.0078, 0.0082, 0.0071,  ..., 0.0083, 0.0077, 0.0075],
           ...,
           [0.0072, 0.0085, 0.0071,  ..., 0.0086, 0.0076, 0.0080],
           [0.0078, 0.0078, 0.0072,  ..., 0.0082, 0.0070, 0.0068],
           [0.0074, 0.0084, 0.0069,  ..., 0.0086, 0.0073, 0.0073]],

          [[0.0089, 0.0081, 0.0074,  ..., 0.0080, 0.0087, 0.0074],
           [0.0078, 0.0073, 0.0071,  ..., 0.0079, 0.0088, 0.0067],
           [0.0070, 0.0071, 0.0078,  ..., 0.0066, 0.0082, 0.0064],
           ...,
           [0.0085, 0.0070, 0.0086,  ..., 0.0073, 0.0094, 0.0066],
           [0.0071, 0.0072, 0.0077,  ..., 0.0076, 0.0083, 0.0063],
           [0.0084, 0.0072, 0.0086,  ..., 0.0075, 0.0079, 0.0066]],

          ...,

          [[0.0085, 0.0086, 0.0072,  ..., 0.0079, 0.0076, 0.0079],
           [0.0079, 0.0075, 0.0075,  ..., 0.0089, 0.0077, 0.0088],
           [0.0077, 0.0081, 0.0079,  ..., 0.0086, 0.0081, 0.0075],
           ...,
           [0.0070, 0.0088, 0.0073,  ..., 0.0089, 0.0073, 0.0078],
           [0.0084, 0.0081, 0.0077,  ..., 0.0079, 0.0078, 0.0072],
           [0.0076, 0.0078, 0.0079,  ..., 0.0080, 0.0075, 0.0069]],

          [[0.0085, 0.0077, 0.0080,  ..., 0.0080, 0.0081, 0.0076],
           [0.0075, 0.0076, 0.0077,  ..., 0.0078, 0.0087, 0.0081],
           [0.0086, 0.0071, 0.0086,  ..., 0.0073, 0.0075, 0.0068],
           ...,
           [0.0084, 0.0074, 0.0084,  ..., 0.0076, 0.0084, 0.0077],
           [0.0080, 0.0068, 0.0079,  ..., 0.0070, 0.0076, 0.0065],
           [0.0086, 0.0071, 0.0081,  ..., 0.0077, 0.0080, 0.0070]],

          [[0.0080, 0.0077, 0.0080,  ..., 0.0075, 0.0074, 0.0078],
           [0.0073, 0.0079, 0.0077,  ..., 0.0077, 0.0072, 0.0082],
           [0.0081, 0.0082, 0.0076,  ..., 0.0077, 0.0078, 0.0078],
           ...,
           [0.0073, 0.0079, 0.0072,  ..., 0.0082, 0.0071, 0.0083],
           [0.0081, 0.0080, 0.0075,  ..., 0.0080, 0.0079, 0.0079],
           [0.0081, 0.0083, 0.0078,  ..., 0.0083, 0.0078, 0.0077]]]]],
       grad_fn=<SoftmaxBackward>)

------------END------------

